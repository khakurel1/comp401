{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pulp in e:\\code\\python\\stock-evaluator\\env\\lib\\site-packages (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scipy in e:\\code\\python\\stock-evaluator\\env\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in e:\\code\\python\\stock-evaluator\\env\\lib\\site-packages (from scipy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pulp\n",
    "%pip install scipy\n",
    "# %pip install pandas\n",
    "# %pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import datetime\n",
    "import pulp\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"AEP\", \"DFSVX\", \"DFLVX\", \"FSAGX\"]\n",
    "GS1_URL = \"https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1318&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=GS1&scale=left&cosd=1953-04-01&coed=2024-02-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Monthly&fam=avg&fgst=lin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2024-03-11&revision_date=2024-03-11&nd=1953-04-01\"\n",
    "\n",
    "START_DATE = \"01/01/1995\"\n",
    "END_DATE = \"09/01/2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AEP': 75.84803009033203, 'DFSVX': 40.84297180175781, 'DFLVX': 42.006439208984375, 'FSAGX': 21.843658447265625}\n"
     ]
    }
   ],
   "source": [
    "from yahoo_fin.stock_info import get_data\n",
    "\n",
    "tickers_data = {}\n",
    "closed_values = {}\n",
    "for t in tickers:\n",
    "    tickers_data[t] = get_data(t, start_date=START_DATE, end_date=END_DATE, index_as_date=False, interval=\"1mo\")\n",
    "    closed_values[t] = tickers_data[t].iloc[-1][\"adjclose\"]\n",
    "\n",
    "print(closed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor t in tickers:\\n    df = pd.read_json(f\"./data/{t}.json\", convert_dates=[\"Date\"])\\n    merged = pd.merge(merged, df[[\"date\", \"Adj Close\"]], on=\"Date\", how=\"inner\")\\n    merged = merged.rename(columns={\"Adj Close\":f\"{t}_Adjusted\"})\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gs1_data = requests.get(GS1_URL)\n",
    "fred = pd.read_csv(io.StringIO(gs1_data.text), parse_dates=[\"DATE\"])\n",
    "fred = fred.rename(columns={\"DATE\":\"date\"})\n",
    "fred = fred.loc[(fred['date'] >= START_DATE) & (fred['date'] < END_DATE)]\n",
    "\n",
    "gs1_val = fred.iloc[-1][\"GS1\"]\n",
    "\n",
    "'''\n",
    "for t in tickers:\n",
    "    df = pd.read_json(f\"./data/{t}.json\", convert_dates=[\"Date\"])\n",
    "    merged = pd.merge(merged, df[[\"date\", \"Adj Close\"]], on=\"Date\", how=\"inner\")\n",
    "    merged = merged.rename(columns={\"Adj Close\":f\"{t}_Adjusted\"})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = fred\n",
    "for t in tickers_data:\n",
    "    df = tickers_data[t]\n",
    "    merged = pd.merge(merged, df[[\"date\", \"adjclose\"]], on=\"date\", how=\"inner\")\n",
    "    merged = merged.rename(columns={\"adjclose\":f\"{t}_Adjusted\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date   GS1  AEP_Adjusted  DFSVX_Adjusted  DFLVX_Adjusted  \\\n",
      "0 1996-01-01  5.09     12.219304        2.448707        3.688004   \n",
      "1 1996-02-01  4.94     11.839611        2.502316        3.741763   \n",
      "2 1996-03-01  5.34     11.688335        2.571488        3.857351   \n",
      "3 1996-04-01  5.54     11.373382        2.704645        3.935062   \n",
      "4 1996-05-01  5.64     11.233397        2.810132        3.999793   \n",
      "\n",
      "   FSAGX_Adjusted  AEP_annual  AEP_excess  DFSVX_annual  DFSVX_excess  \\\n",
      "0       11.230021    0.354033    0.283533      0.275800      0.205300   \n",
      "1       11.678018    0.355529    0.288529      0.259311      0.192311   \n",
      "2       11.958015    0.403813    0.339513      0.278872      0.214572   \n",
      "3       12.242317    0.324276    0.261576      0.299164      0.236464   \n",
      "4       13.577686    0.250692    0.190692      0.313526      0.253526   \n",
      "\n",
      "   DFLVX_annual  DFLVX_excess  FSAGX_annual  FSAGX_excess  \n",
      "0      0.391166      0.320666      0.462142      0.391642  \n",
      "1      0.335715      0.268715      0.470174      0.403174  \n",
      "2      0.361859      0.297559      0.305125      0.240825  \n",
      "3      0.335317      0.272617      0.341199      0.278499  \n",
      "4      0.290045      0.230045      0.457235      0.397235  \n"
     ]
    }
   ],
   "source": [
    "shifted = merged.shift(12)\n",
    "for t in tickers:\n",
    "    col_to_substract = f\"{t}_Adjusted\"\n",
    "    merged[f\"{t}_annual\"] = (merged[col_to_substract]- shifted[col_to_substract]) / shifted[col_to_substract]\n",
    "    merged[f\"{t}_excess\"] = merged[f\"{t}_annual\"] - merged.shift(12)[\"GS1\"]/100\n",
    "\n",
    "print(merged.shift(-12).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AEP': (0.07754665280376574, 0.19979385241599257), 'DFSVX': (0.10946865468284153, 0.24398202765486768), 'DFLVX': (0.08998956545061457, 0.19464668574460334), 'FSAGX': (0.05298878807301346, 0.30888410177569214)}\n"
     ]
    }
   ],
   "source": [
    "return_dist = {}\n",
    "for t in tickers:\n",
    "    cleaned = merged[merged[f\"{t}_excess\"].notna()]\n",
    "    return_dist[t] = (cleaned[f\"{t}_excess\"].mean(), cleaned[f\"{t}_excess\"].std())\n",
    "\n",
    "print(return_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AEP     DFSVX     DFLVX     FSAGX\n",
      "AEP    1.000000  0.393708  0.561141 -0.035950\n",
      "DFSVX  0.393708  1.000000  0.902942  0.091180\n",
      "DFLVX  0.561141  0.902942  1.000000  0.026616\n",
      "FSAGX -0.035950  0.091180  0.026616  1.000000\n"
     ]
    }
   ],
   "source": [
    "## Calulating correlation matrix\n",
    "cols = [f\"{col}_excess\" for col in tickers]\n",
    "rename_dict = {}\n",
    "\n",
    "for col in cols:\n",
    "    rename_dict[col] = col.replace(\"_excess\",\"\")\n",
    "    \n",
    "corr = merged[cols].corr()\n",
    "corr = corr.rename(index=rename_dict, columns=rename_dict)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            AEP     DFSVX     DFLVX     FSAGX\n",
      "AEP    0.039918  0.019192  0.021822 -0.002219\n",
      "DFSVX  0.019192  0.059527  0.042881  0.006872\n",
      "DFLVX  0.021822  0.042881  0.037887  0.001600\n",
      "FSAGX -0.002219  0.006872  0.001600  0.095409\n"
     ]
    }
   ],
   "source": [
    "## Calulating variance-covariance\n",
    "var_covar = corr.copy()\n",
    "for row_label, row_data in var_covar.iterrows():\n",
    "    row_std = return_dist[row_label][1]\n",
    "    for col_label in var_covar.columns:\n",
    "        col_std = return_dist[col_label][1]\n",
    "        row_data[col_label] = row_data[col_label]*row_std*col_std\n",
    "\n",
    "print(var_covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03991758  0.01919173  0.02182234 -0.00221858]\n",
      " [ 0.01919173  0.05952723  0.04288097  0.00687155]\n",
      " [ 0.02182234  0.04288097  0.03788733  0.00160024]\n",
      " [-0.00221858  0.00687155  0.00160024  0.09540939]]\n",
      "0.08606170773875482\n"
     ]
    }
   ],
   "source": [
    "weights = np.array(\n",
    "    [[0.36656],\n",
    "    [0.32678],\n",
    "    [0.15174],\n",
    "    [0.15492]]\n",
    "    )\n",
    "\n",
    "\n",
    "print(var_covar.to_numpy())\n",
    "res = weights.T @ np.power(np.absolute(var_covar.to_numpy()) @ weights,0.5)\n",
    "sd = res[0,0]\n",
    "\n",
    "risk_prem = np.dot(weights.T, np.array([[return_dist[r][0]] for r in return_dist.keys()]))[0,0]\n",
    "sharpie_ratio  = risk_prem/sd\n",
    "print(risk_prem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min SD for efficient frontier: 0.15138863364510582 [0.38243598 0.         0.37501673]\n",
      "risk prem for efficient frontier 1st line. [0.07625651]\n"
     ]
    }
   ],
   "source": [
    "## Here we do the optimization operations.\n",
    "\n",
    "# Calulcate the min variance portfolio for effecient frontier's SD and riskpremium\n",
    "import scipy.optimize as optimize\n",
    "import math\n",
    "import json \n",
    "\n",
    "def calc_risk_premium(wts, return_distr_risk_prem):\n",
    "    risk_prem = (wts @ return_distr_risk_prem).item()\n",
    "    return risk_prem\n",
    "\n",
    "def extend_x(x):\n",
    "    return [x[0],x[1],x[2],1-x[0]-x[1]-x[2]]\n",
    "\n",
    "covar = var_covar.to_numpy()\n",
    "def objective_function(x):\n",
    "    iter_weights = np.array(extend_x(x))\n",
    "    return np.power((iter_weights @ covar) @ iter_weights.T ,0.5).item()\n",
    "\n",
    "def constraint_risk_prem(x, target):\n",
    "    iter_weights = np.array(extend_x(x))\n",
    "    return_distr_risk_premium = np.array([[return_dist[r][0]] for r in return_dist.keys()])\n",
    "    risk_prem = calc_risk_premium(iter_weights, return_distr_risk_premium)\n",
    "    # print(target, risk_prem)\n",
    "    return risk_prem - target\n",
    "\n",
    "def total_one(x):\n",
    "    return (x[0] + x[1] + x[2]) - 1\n",
    "\n",
    "# Create the constraint object\n",
    "total_one_constr = optimize.NonlinearConstraint(total_one, 0, 0)\n",
    "bounds = [(0,None),(0,None),(0,None)]\n",
    "\n",
    "weights_map = {}\n",
    "# Initial guess\n",
    "initial_values = np.array([0.12, 0.43, 0.23])\n",
    "\n",
    "# Call the minimize function\n",
    "result = scipy.optimize.minimize(objective_function, initial_values, method='SLSQP', constraints=[], bounds=bounds)\n",
    "\n",
    "# Print the result\n",
    "print(\"min SD for efficient frontier:\", result.fun, result.x)\n",
    "# print(\"optimized weights: \", result.x)\n",
    "\n",
    "risk_prem = np.array(extend_x(result.x)) @ np.array([[return_dist[r][0]] for r in return_dist.keys()])\n",
    "print(\"risk prem for efficient frontier 1st line.\",risk_prem)\n",
    "\n",
    "changing_wts = result.x\n",
    "\n",
    "weights_map[f\"{risk_prem.item() * 100}\"] = {\n",
    "    \"wts\": result.x,\n",
    "    \"sd\": result.fun\n",
    "}\n",
    "\n",
    "# print(np.array(extend_x(result.x)), np.array([[return_dist[r][0]] for r in return_dist.keys()]), )\n",
    "next_risk_prem = math.ceil(risk_prem.item() * 100)\n",
    "for val in range(next_risk_prem, next_risk_prem + 8):\n",
    "    target_risk_prem = val * 0.01\n",
    "    # print(target_risk_prem)\n",
    "    risk_prem_eq_val = optimize.NonlinearConstraint(lambda x: constraint_risk_prem(x,target_risk_prem), 0, 0)\n",
    "    result = scipy.optimize.minimize(objective_function, changing_wts, method='SLSQP', constraints=[risk_prem_eq_val], bounds=bounds,  tol=1e-6, options={'maxiter': 1000})\n",
    "    changing_wts = result.x\n",
    "    # print(\"min sd:\", result.fun, result.x)\n",
    "   \n",
    "    weights_map[f\"{val}\"] = {\n",
    "        \"sd\": result.fun,\n",
    "        \"wts\": result.x.tolist()\n",
    "        }\n",
    "\n",
    "    risk_prem = np.array(extend_x(changing_wts)) @ np.array([[return_dist[r][0]] for r in return_dist.keys()])\n",
    "    # print(risk_prem[0])\n",
    "\n",
    "data = {\n",
    "    \"varcovar\": var_covar.to_numpy().tolist(),\n",
    "    \"efficient_frontier\": weights_map,\n",
    "    \"return_distribution\": np.array([[return_dist[r][0]] for r in return_dist.keys()]).tolist(),\n",
    "    \"closed_values\": closed_values,\n",
    "    \"gs1_value\": gs1_val*0.01,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# print(json.dumps(data[\"efficient_frontier\"],cls=NumpyEncoder))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Sharpie ratio: 0.526918768545048 [0.3758766  0.33213246 0.13739157]\n",
      "risk prem for efficient frontier 1st line. [0.08606191]\n",
      "sd. [0.1633305]\n"
     ]
    }
   ],
   "source": [
    "## Getting value for maximum sharpie ratio\n",
    "\n",
    "import scipy.optimize as optimize\n",
    "import math\n",
    "import json \n",
    "\n",
    "def calc_risk_premium(wts, return_distr_risk_prem):\n",
    "    risk_prem = (wts @ return_distr_risk_prem).item()\n",
    "    return risk_prem\n",
    "\n",
    "def extend_x(x):\n",
    "    return [x[0],x[1],x[2],1-x[0]-x[1]-x[2]]\n",
    "\n",
    "covar = var_covar.to_numpy()\n",
    "def objective_function(x):\n",
    "    iter_weights = np.array(extend_x(x))\n",
    "    return_distr_risk_premium = np.array([[return_dist[r][0]] for r in return_dist.keys()])\n",
    "    risk_prem = calc_risk_premium(iter_weights, return_distr_risk_premium)\n",
    "    sd = np.power((iter_weights @ covar) @ iter_weights.T ,0.5).item()\n",
    "    return sd/risk_prem\n",
    "\n",
    "# Create the constraint object\n",
    "bounds = [(0,None),(0,None),(0,None)]\n",
    "# Initial guess\n",
    "initial_values = np.array([0.12, 0.43, 0.23])\n",
    "\n",
    "# Call the minimize function\n",
    "result = scipy.optimize.minimize(objective_function, initial_values, method='SLSQP', constraints=[], bounds=bounds)\n",
    "\n",
    "# Print the result\n",
    "print(\"max Sharpie ratio:\", 1/result.fun, result.x)\n",
    "# print(\"optimized weights: \", result.x)\n",
    "\n",
    "risk_prem = np.array(extend_x(result.x)) @ np.array([[return_dist[r][0]] for r in return_dist.keys()])\n",
    "print(\"risk prem for efficient frontier 1st line.\",risk_prem)\n",
    "efficient_sd = result.fun * risk_prem\n",
    "print(\"sd.\",efficient_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'y': 0.0, 'sd': 0.0, 'return': 0.053700000000000005}, {'y': 0.1, 'sd': 0.01633305013745721, 'return': 0.062306190665013486}, {'y': 0.2, 'sd': 0.03266610027491442, 'return': 0.07091238133002696}, {'y': 0.30000000000000004, 'sd': 0.048999150412371635, 'return': 0.07951857199504045}, {'y': 0.4, 'sd': 0.06533220054982884, 'return': 0.08812476266005392}, {'y': 0.5, 'sd': 0.08166525068728604, 'return': 0.0967309533250674}, {'y': 0.6000000000000001, 'sd': 0.09799830082474327, 'return': 0.1053371439900809}, {'y': 0.7000000000000001, 'sd': 0.11433135096220047, 'return': 0.11394333465509437}, {'y': 0.8, 'sd': 0.13066440109965768, 'return': 0.12254952532010785}, {'y': 0.9, 'sd': 0.14699745123711488, 'return': 0.13115571598512132}, {'y': 1.0, 'sd': 0.16333050137457208, 'return': 0.1397619066501348}, {'y': 1.1, 'sd': 0.1796635515120293, 'return': 0.1483680973151483}]\n"
     ]
    }
   ],
   "source": [
    "## Getting values for complete portfolio\n",
    "\n",
    "## y, sd, return\n",
    "\n",
    "complete_portfolio = []\n",
    "for i in range(0,12):\n",
    "    sd = i * 0.1 * efficient_sd\n",
    "        \n",
    "    if i==0 :\n",
    "        ex_return = gs1_val * 0.01\n",
    "    else:\n",
    "        ex_return = i * 0.1 * risk_prem + gs1_val * 0.01\n",
    "\n",
    "\n",
    "    complete_portfolio.append({\n",
    "        \"y\": i * 0.1,\n",
    "        \"sd\": sd.item(),\n",
    "        \"return\": ex_return.item(),\n",
    "    })\n",
    "\n",
    "print(complete_portfolio)\n",
    "\n",
    "\n",
    "price_of_risk = risk_prem/efficient_sd**2\n",
    "risk_aver = 4 # this is also an arbitraty value.\n",
    "y = price_of_risk/risk_aver\n",
    "\n",
    "DGS1 = gs1_data*0.01     # This is the cuurent GS1 value for the current month.\n",
    "E_rf = (1-y)*DGS1\n",
    "E_rP = y * (DGS1+risk_prem)\n",
    "\n",
    "expected_return =  E_rf+E_rP\n",
    "riskiness = y * efficient_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.optimize\n",
    "\n",
    "\n",
    "# covar = var_covar.to_numpy()\n",
    "\n",
    "# def calc_risk_premium(wts, return_distr_risk_prem):\n",
    "#     risk_prem = wts @ return_distr_risk_prem\n",
    "#     return risk_prem\n",
    "\n",
    "# def objective_function(x):\n",
    "#     iter_weights = np.array(x)\n",
    "#     return np.power((iter_weights @ covar) @ iter_weights.T ,0.5).item() - total_one(x)\n",
    "\n",
    "# def constraint_risk_prem(x, target):\n",
    "#     iter_weights = np.array(x)\n",
    "#     return_distr_risk_premium = np.array([[return_dist[r][0]] for r in return_dist.keys()])\n",
    "#     risk_prem = calc_risk_premium(iter_weights, return_distr_risk_premium)\n",
    "#     return risk_prem - target\n",
    "\n",
    "# def total_one(x):\n",
    "#     return (x[0] + x[1] + x[2] + x[3]) - 1\n",
    "\n",
    "# # Define the bounds for each variable\n",
    "# bounds = [(0, 1), (0, 1), (0, 1), (0, 1)]\n",
    "\n",
    "# # Initial guess\n",
    "# initial_values = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "# # Define the constraint function for fmin_l_bfgs_b\n",
    "# def constraint_function(x):\n",
    "#     # Assuming a target risk premium\n",
    "#     target_risk_premium = 0.05 # Example value\n",
    "#     risk_prem_diff = constraint_risk_prem(x, target_risk_premium)\n",
    "#     total_one_diff = total_one(x)\n",
    "#     return (risk_prem_diff, total_one_diff)\n",
    "\n",
    "# # Call the fmin_l_bfgs_b function\n",
    "# result = scipy.optimize.fmin_l_bfgs_b(objective_function, initial_values, bounds=bounds, approx_grad=True)\n",
    "\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
